{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b8d297-8dc1-4017-8680-ea2a23060888",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400bbff4-7a8e-4c57-99a5-3d2a7014f257",
   "metadata": {},
   "source": [
    "## Install important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a264f8-bbe9-454d-9d97-6b46af0afaad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install -U sagemaker\n",
    "!pip3 install polars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f75c0-0e40-47eb-a3cf-abce9040900e",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81218d24-0910-4a1c-bd90-eae9c92b1e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import boto3\n",
    "import sagemaker\n",
    "import polars as pl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb67f828-d574-4a92-b7b7-b20d50df0ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "region = sagemaker.Session().boto_region_name\n",
    "staging_bucket = 'db-project-staging-area'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c2876df-01e1-4bd0-b515-4fe651923278",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3resource = boto3.resource(\"s3\")\n",
    "s3client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b8282b6-ac0b-4a90-afc4-ef1c3d84898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_buffer(bucket, key):\n",
    "    buffer = io.BytesIO()\n",
    "    bucket.download_fileobj(key, buffer)\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9087ae6f-3d84-4ef2-a0b0-15c54134431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "locationcategory_key = [x['Key'] for x in s3client.list_objects_v2(Bucket=staging_bucket, Prefix='locationcategory/')[\"Contents\"]][0]\n",
    "stationrelation_key = [x['Key'] for x in s3client.list_objects_v2(Bucket=staging_bucket, Prefix='stationrelation/')[\"Contents\"]][0]\n",
    "location_key = [x['Key'] for x in s3client.list_objects_v2(Bucket=staging_bucket, Prefix='location/')[\"Contents\"]][0]\n",
    "datatype_key = [x['Key'] for x in s3client.list_objects_v2(Bucket=staging_bucket, Prefix='datatype/')[\"Contents\"]][0]\n",
    "station_key = [x['Key'] for x in s3client.list_objects_v2(Bucket=staging_bucket, Prefix='station/')[\"Contents\"]][0]\n",
    "date_key = [x['Key'] for x in s3client.list_objects_v2(Bucket=staging_bucket, Prefix='date/')[\"Contents\"]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f59feb91-4a37-467f-9d4c-e37d400cfaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(get_s3_buffer(s3resource.Bucket(staging_bucket), locationcategory_key)).to_csv(os.path.join(\"staging_data\", \"locationcategory.csv\"), index=False)\n",
    "pd.read_parquet(get_s3_buffer(s3resource.Bucket(staging_bucket), stationrelation_key)).to_csv(os.path.join(\"staging_data\", \"stationrelation.csv\"), index=False)\n",
    "pd.read_parquet(get_s3_buffer(s3resource.Bucket(staging_bucket), location_key)).to_csv(os.path.join(\"staging_data\", \"location.csv\"), index=False)\n",
    "pd.read_parquet(get_s3_buffer(s3resource.Bucket(staging_bucket), datatype_key)).to_csv(os.path.join(\"staging_data\", \"datatype.csv\"), index=False)\n",
    "pd.read_parquet(get_s3_buffer(s3resource.Bucket(staging_bucket), station_key)).to_csv(os.path.join(\"staging_data\", \"station.csv\"), index=False)\n",
    "pd.read_parquet(get_s3_buffer(s3resource.Bucket(staging_bucket), date_key)).to_csv(os.path.join(\"staging_data\", \"date.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ecac963-ee2a-4a0a-8386-b5a6ac032430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year=2010 done\n",
      "year=2011 done\n",
      "year=2012 done\n",
      "year=2013 done\n",
      "year=2014 done\n",
      "year=2015 done\n",
      "year=2016 done\n",
      "year=2017 done\n",
      "year=2018 done\n",
      "year=2019 done\n",
      "year=2020 done\n",
      "year=2021 done\n",
      "year=2022 done\n"
     ]
    }
   ],
   "source": [
    "station = pd.read_csv(os.path.join(\"staging_data\", \"station.csv\")).drop(columns=\"name\")\n",
    "date = pd.read_csv(os.path.join(\"staging_data\", \"date.csv\")).drop(columns=[\"month_name\", \"day_name\", \"date\", \"weekday\", \"day\"]).astype({\"is_leap_year\": \"int\"})\n",
    "date[\"year\"] = date[\"year\"] - 2010\n",
    "data = None\n",
    "for i in range(2010, 2023):\n",
    "    data_keys = [x['Key'] for x in s3client.list_objects_v2(Bucket=staging_bucket, Prefix=f'data/year={i}/')[\"Contents\"]]\n",
    "    df = None\n",
    "    for key in data_keys:\n",
    "        tdf = pd.read_parquet(get_s3_buffer(s3resource.Bucket(staging_bucket), key))\n",
    "        if df is None:\n",
    "            df = tdf\n",
    "        else:\n",
    "            df = pd.concat([df, tdf])\n",
    "    df = df.pivot(index=[\"stationid\", \"dateid\"], columns=\"datatypeid\", values=\"value\").reset_index().fillna(0).astype({\"TMAX\": \"int\", \"TMIN\": \"int\", \"PRCP\": \"int\", \"SNOW\": \"int\", \"SNWD\": \"int\"})\n",
    "    df = df.merge(date, how=\"left\", on=\"dateid\").drop(columns=[\"dateid\"])\n",
    "    df = df.merge(station, how=\"left\", left_on=\"stationid\", right_on=\"id\").drop(columns=[\"stationid\", \"id\"])\n",
    "    df.to_csv(os.path.join(\"staging_data\", f\"clean{i}.csv\"), index=False)\n",
    "    print(f\"year={i} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac6d3461-63a1-4b8c-962d-9b55b918d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "files = [os.path.join(\"staging_data\", f\"clean{i}.csv\") for i in range(2010, 2023)]\n",
    "\n",
    "with open(os.path.join(\"staging_data\", f\"training_data.csv\"), \"w\") as datafile:\n",
    "    writer = csv.writer(datafile)\n",
    "    for i, filepath in enumerate(files):\n",
    "        with open(filepath, \"r\") as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            if i != 0:\n",
    "                _ = next(reader, None)\n",
    "            else:\n",
    "                writer.writerow(next(reader, None))\n",
    "            for line in reader:\n",
    "                writer.writerow(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
